import AVFoundation
import BookMatchAPI
import BookMatchCore
import BookMatchStrategy
import CoreFoundation
import CoreML
import RxSwift
import UIKit
import Vision

@_exported import struct BookMatchCore.OwnedBook

/// ë„ì„œ `ë§¤ì¹­` ê¸°ëŠ¥ì˜ í•µì‹¬ ëª¨ë“ˆì…ë‹ˆë‹¤.
/// ì‚¬ìš©ìì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ê³ , ë„ì„œ ê²€ìƒ‰, `ë§¤ì¹­` ê¸°ëŠ¥ì„ ì¡°ìœ¨í•©ë‹ˆë‹¤.
public final class BookMatchKit: BookMatchable {
    // MARK: - Properties

    private let imageStrategy = VisionImageStrategy()
    private let naverAPI: NaverAPI
    private let imageDownloadAPI: ImageDownloadAPI
    private let disposeBag = DisposeBag()

    // MARK: - Lifecycle

    public init(
        naverClientId: String,
        naverClientSecret: String
    ) {
        let config = APIConfiguration(
            naverClientId: naverClientId,
            naverClientSecret: naverClientSecret,
            openAIApiKey: ""
        )

        naverAPI = NaverAPI(configuration: config)
        imageDownloadAPI = ImageDownloadAPI(configuration: config)
    }

    // MARK: - Functions

    // MARK: - Public

    /// `OCRë¡œ ì¸ì‹ëœ í…ìŠ¤íŠ¸ ë°ì´í„°ì™€ ì´ë¯¸ì§€`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ë„ì„œë¥¼ `ë§¤ì¹­`í•©ë‹ˆë‹¤.
    ///
    /// - Parameters:
    ///   - image: ë„ì„œ í‘œì§€ ì´ë¯¸ì§€
    /// - Returns: ë§¤ì¹­ëœ ë„ì„œ ì •ë³´ ë˜ëŠ” nil
    /// - Throws: ì´ˆê¸° ë‹¨ì–´ë¶€í„° ê²€ìƒ‰ëœ ê²°ê³¼ê°€ ë‚˜ì˜¤ì§€ ì•Šì„ ë•Œ
    public func matchBook(_ image: UIImage) async throws -> BookItem? {
        let textData = try await extractText(from: image)
        let searchResults = try await fetchSearchResults(from: textData)

        // ê²€ìƒ‰ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ìœ ì‚¬ë„ ê³„ì‚° ìˆ˜í–‰
        guard !searchResults.isEmpty else {
            throw BookMatchError.noMatchFound
        }

        var similarityResults = [(BookItem, Double)]()

        for book in searchResults {
            let bookImage = try await imageDownloadAPI.downloadImage(from: book.image).value
            let similarity = try await imageStrategy.calculateSimilarity(image, bookImage).value

            similarityResults.append((book, similarity))
        }

        let sortedResults = similarityResults.sorted { $0.1 > $1.1 }

        return sortedResults[0].0
    }

    /// `OCRë¡œ ê²€ì¶œëœ í…ìŠ¤íŠ¸ ë°°ì—´`ë¡œ ë„ì„œë¥¼ ê²€ìƒ‰í•©ë‹ˆë‹¤.
    /// - Note:``matchBook(_:, image:)`` ë©”ì„œë“œì— ì‚¬ìš©ë©ë‹ˆë‹¤.
    ///
    /// - Parameters:
    ///   - sourceBook: ê²€ìƒ‰í•  ë„ì„œì˜ ê¸°ë³¸ ì •ë³´
    /// - Returns: ê²€ìƒ‰ëœ ë„ì„œ ëª©ë¡
    /// - Throws: BookMatchError
    private func fetchSearchResults(from textData: [String]) async throws -> [BookItem] {
        var searchResults = [BookItem]()
        var previousResults = [BookItem]()
        var currentIndex = 0
        var currentQuery = ""

        while currentIndex < textData.count {
            if currentQuery.isEmpty {
                currentQuery = textData[currentIndex]
            } else {
                currentQuery = [currentQuery, textData[currentIndex]].joined(separator: " ")
            }

            try await Task.sleep(nanoseconds: 500_000_000) // Rate limit ë°©ì§€
            let results = try await naverAPI.searchBooks(query: currentQuery, limit: 10).value

            // ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ì €ì¥
            if !results.isEmpty {
                previousResults = results
            }

            // ê²€ìƒ‰ ê²°ê³¼ê°€ 3ê°œ ì´í•˜ë©´ ìµœì ì˜ ì¿¼ë¦¬ë¡œ íŒë‹¨í•˜ê³  ì¤‘ë‹¨
            if results.count <= 3 {
                searchResults = previousResults // ì´ì „ ê²€ìƒ‰ ê²°ê³¼ ì‚¬ìš©
                break
            }

            // ë§ˆì§€ë§‰ ë‹¨ì–´ ê·¸ë£¹ê¹Œì§€ ë„ë‹¬í–ˆëŠ”ë°ë„ 3ê°œ ì´í•˜ê°€ ì•ˆ ëœ ê²½ìš°
            if currentIndex == textData.count - 1 {
                searchResults = results.isEmpty ? previousResults : results
                break
            }

            currentIndex += 1
        }

        return searchResults
    }

    // MARK: - OCR Logic

    /// ì´ë¯¸ì§€ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ê³ , ì¶”ì¶œëœ í…ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    /// - Parameter image: í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•  ì´ë¯¸ì§€
    /// - Returns: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ë°°ì—´
    private func extractText(from image: UIImage) async throws -> [String] {
        print("ğŸ“Œ extractText ì‹¤í–‰ë¨!")

        return await withCheckedContinuation { continuation in
            detectBookElements(in: image) { extractedTexts in
                print("ğŸ“Œ detectBookElements ê²°ê³¼: \(extractedTexts)")
                continuation.resume(returning: extractedTexts)
            }
        }
    }

    /// ê°ì§€ëœ ë°”ìš´ë”© ë°•ìŠ¤ë¥¼ í™•ì¥í•˜ì—¬ OCR ì •í™•ë„ë¥¼ ë†’ì„
    private func expandBoundingBox(_ boundingBox: CGRect, factor: CGFloat) -> CGRect {
        let x = boundingBox.origin.x - (boundingBox.width * (factor - 1)) / 2
        let y = boundingBox.origin.y - (boundingBox.height * (factor - 1)) / 2
        let width = boundingBox.width * factor
        let height = boundingBox.height * factor

        return CGRect(
            x: max(0, x),
            y: max(0, y),
            width: min(1, width),
            height: min(1, height)
        )
    }

    /// ê°ì§€ëœ ì˜ì—­ì„ í¬ë¡­í•˜ì—¬ OCR ì •í™•ë„ë¥¼ ë†’ì„
    private func cropImage(_ image: UIImage, to boundingBox: CGRect) -> UIImage {
        guard let cgImage = image.cgImage else {
            print("âš ï¸ ì›ë³¸ ì´ë¯¸ì§€ì˜ CGImageë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŒ, ì›ë³¸ ì´ë¯¸ì§€ ë°˜í™˜")
            return image
        }
        let width = CGFloat(cgImage.width)
        let height = CGFloat(cgImage.height)

        let cropRect = CGRect(
            x: boundingBox.origin.x * width,
            y: boundingBox.origin.y * height,
            width: boundingBox.width * width,
            height: boundingBox.height * height
        )

        guard let croppedCGImage = cgImage.cropping(to: cropRect) else {
            print("âš ï¸ ì´ë¯¸ì§€ í¬ë¡­ ì‹¤íŒ¨, ì›ë³¸ ì´ë¯¸ì§€ë¡œ OCR ì§„í–‰")
            return image
        }
        return UIImage(cgImage: croppedCGImage)
    }

    /// CoreMLì„ ì‚¬ìš©í•˜ì—¬ ì±… ì œëª©ì„ ì¸ì‹ í›„ OCR ì‹¤í–‰
    private func detectBookElements(in image: UIImage, completion: @escaping ([String]) -> Void) {
        print("ğŸ“Œ detectBookElements ì‹¤í–‰ë¨!")

        guard let model = try? VNCoreMLModel(for: MyObjectDetector5_1().model) else {
            print("âš ï¸ CoreML ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨, OCR ê°•ì œ ì‹¤í–‰")
            performOCR(on: image, completion: completion)
            return
        }

        let request = VNCoreMLRequest(model: model) { request, error in
            if let error {
                print("âš ï¸ Vision ìš”ì²­ ì‹¤íŒ¨: \(error.localizedDescription), OCR ê°•ì œ ì‹¤í–‰")
                self.performOCR(on: image, completion: completion)
                return
            }

            guard let results = request.results as? [VNRecognizedObjectObservation] else {
                print("âš ï¸ Vision ê²°ê³¼ ì—†ìŒ, OCR ê°•ì œ ì‹¤í–‰")
                self.performOCR(on: image, completion: completion)
                return
            }

            // âœ… Confidence Threshold ì™„í™”
            let filteredResults = results.filter { $0.confidence > 0.3 } // ì‹ ë¢°ë„ 30% ì´ìƒ
            if filteredResults.isEmpty {
                print("âš ï¸ ì‹ ë¢°ë„ ë‚®ìŒ, OCR ê°•ì œ ì‹¤í–‰")
                self.performOCR(on: image, completion: completion)
                return
            }

            print("ğŸ“š ê°ì§€ëœ ê°ì²´ ìˆ˜ (Threshold ì ìš©): \(filteredResults.count)")

            var extractedTexts: [String] = []
            let dispatchGroup = DispatchGroup()

            for observation in filteredResults {
                let detectedLabel = observation.labels.first?.identifier ?? "Unknown"
                print("ğŸ” ê°ì§€ëœ ê°ì²´: \(detectedLabel)")

                if detectedLabel == "titles-or-authors" || detectedLabel == "book-title" {
                    dispatchGroup.enter()

                    let expandedBox = self.expandBoundingBox(observation.boundingBox, factor: 1.2)
                    let croppedImage = self.cropImage(image, to: expandedBox)

                    // âœ… ì›ë³¸ê³¼ í¬ë¡­ëœ ì´ë¯¸ì§€ ëª¨ë‘ OCR ì‹¤í–‰í•˜ì—¬ ë” ì¢‹ì€ ê²°ê³¼ ì„ íƒ
                    self.performOCR(on: croppedImage) { croppedText in
                        self.performOCR(on: image) { originalText in
                            let finalText = croppedText.isEmpty ? originalText : croppedText
                            print("âœ… OCR ì‹¤í–‰ ì™„ë£Œ, ìµœì¢… ê²°ê³¼: \(finalText)")
                            extractedTexts.append(contentsOf: finalText)
                            dispatchGroup.leave()
                        }
                    }
                }
            }

            dispatchGroup.notify(queue: .main) {
                print("ğŸ“‘ ìµœì¢… OCR í…ìŠ¤íŠ¸: \(extractedTexts)")
                if extractedTexts.isEmpty {
                    print("âš ï¸ OCR ì‹¤íŒ¨, ì›ë³¸ ì´ë¯¸ì§€ë¡œ ìµœì¢… OCR ì‹¤í–‰")
                    self.performOCR(on: image, completion: completion)
                } else {
                    completion(extractedTexts)
                }
            }
        }

        do {
            let handler = VNImageRequestHandler(cgImage: image.cgImage!, options: [:])
            try handler.perform([request])
        } catch let error as NSError {
            print("âš ï¸ Vision Request Error: \(error.localizedDescription), OCR ê°•ì œ ì‹¤í–‰")
            performOCR(on: image, completion: completion)
        }
    }

    private func convertToGrayscale(_ image: UIImage) -> UIImage? {
        guard let ciImage = CIImage(image: image) else {
            return nil
        }

        let filter = CIFilter(name: "CIColorControls")
        filter?.setValue(ciImage, forKey: kCIInputImageKey)
        filter?.setValue(1.5, forKey: kCIInputContrastKey) // ëŒ€ë¹„ ì¦ê°€
        filter?.setValue(0.0, forKey: kCIInputSaturationKey) // ì±„ë„ ì œê±° (í‘ë°±)

        guard let outputImage = filter?.outputImage else {
            return nil
        }
        let context = CIContext(options: nil)
        guard let cgImage = context.createCGImage(outputImage, from: outputImage.extent)
        else {
            return nil
        }

        return UIImage(cgImage: cgImage)
    }

    private func performOCR(on image: UIImage, completion: @escaping ([String]) -> Void) {
        print("ğŸ“Œ performOCR ì‹¤í–‰ë¨!")

        guard let preprocessedImage = convertToGrayscale(image),
              let cgImage = preprocessedImage.cgImage else {
            print("âš ï¸ ëŒ€ë¹„ ì¡°ì • ì‹¤íŒ¨, ì›ë³¸ ì´ë¯¸ì§€ ì‚¬ìš©")
            completion([])
            return
        }

        let request = VNRecognizeTextRequest { request, error in
            if let error {
                print("âš ï¸ OCR ì˜¤ë¥˜ ë°œìƒ: \(error.localizedDescription)")
                completion([])
                return
            }

            guard let observations = request.results as? [VNRecognizedTextObservation],
                  !observations.isEmpty else {
                print("âš ï¸ OCR ê²°ê³¼ ì—†ìŒ")
                completion([])
                return
            }

            // í…ìŠ¤íŠ¸ í¬ê¸° ìˆœì„œëŒ€ë¡œ ì •ë ¬
            let sortedObservations = observations.sorted { obs1, obs2 in
                let size1 = obs1.boundingBox.width * obs1.boundingBox.height
                let size2 = obs2.boundingBox.width * obs2.boundingBox.height
                return size1 > size2 // í¬ê¸°ê°€ í° ìˆœì„œëŒ€ë¡œ ì •ë ¬
            }

            // ì •ë ¬ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            let recognizedText = sortedObservations.compactMap { $0.topCandidates(1).first?.string }
            print("âœ… OCR ì¸ì‹ëœ í…ìŠ¤íŠ¸ (í¬ê¸° ìˆœì„œëŒ€ë¡œ): \(recognizedText)")
            completion(recognizedText)
        }

        request.recognitionLanguages = ["ko", "en"]
        request.usesLanguageCorrection = true
        request.minimumTextHeight = 0.005

        let requestHandler = VNImageRequestHandler(
            cgImage: preprocessedImage.cgImage!,
            options: [:]
        )
        do {
            print("ğŸ“ OCR ì‹¤í–‰ ì¤‘...")
            try requestHandler.perform([request])
        } catch {
            print("âš ï¸ OCR ìš”ì²­ ì‹¤íŒ¨: \(error.localizedDescription)")
            completion([])
        }
    }
}
