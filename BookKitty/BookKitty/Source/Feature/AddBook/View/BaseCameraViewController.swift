import AVFoundation
import CoreML
import DesignSystem
import RxCocoa
import RxSwift
import SnapKit
import Then
import UIKit
import Vision

class BaseCameraViewController: BaseViewController, AVCapturePhotoCaptureDelegate {
    // MARK: - Properties

    open var captureButton: UIButton = CircleIconButton(iconId: "camera.fill")

    var captureSession = AVCaptureSession()
    var previewLayer: AVCaptureVideoPreviewLayer?
    var captureOutput = AVCapturePhotoOutput()

    let cameraView = UIView().then { $0.backgroundColor = .black }

    var ocrTextHandler: ((String) -> Void)? // OCR ê²°ê³¼ ì „ë‹¬ìš© í´ë¡œì €

    // MARK: - Lifecycle

    override func viewDidLoad() {
        super.viewDidLoad()
        checkCameraPermission { granted in
            if granted {
                self.setupCamera()
            } else {
                self.showPermissionAlert()
            }
        }
        bindUI()
    }

    override func viewDidLayoutSubviews() {
        super.viewDidLayoutSubviews()
        DispatchQueue.main.async {
            self.previewLayer?.frame = self.cameraView.bounds
        }
    }

    // MARK: - Functions

    func capturePhoto() {
        let settings = AVCapturePhotoSettings()
        captureOutput.capturePhoto(with: settings, delegate: self)
    }

    func photoOutput(
        _: AVCapturePhotoOutput,
        didFinishProcessingPhoto photo: AVCapturePhoto,
        error: Error?
    ) {
        guard error == nil, let imageData = photo.fileDataRepresentation(),
              let image = UIImage(data: imageData) else {
            showCaptureFailurePopup()
            return
        }
        handleCapturedImage(image)
    }

    func handleCapturedImage(_ image: UIImage) {
        // ì´ë¯¸ì§€ ë¦¬ì‚¬ì´ì¦ˆ (OCR ì •í™•ë„ í–¥ìƒì„ ìœ„í•´)
        let resizedImage = image.resized(toWidth: 1024)
        detectBookElements(in: resizedImage!) // OCR ë° ê°ì²´ ê°ì§€ ì‹¤í–‰
    }

    private func checkCameraPermission(completion: @escaping (Bool) -> Void) {
        switch AVCaptureDevice.authorizationStatus(for: .video) {
        case .authorized:
            completion(true)
        case .notDetermined:
            AVCaptureDevice.requestAccess(for: .video) { granted in
                DispatchQueue.main.async { completion(granted) }
            }
        default:
            completion(false)
        }
    }

    private func showPermissionAlert() {
        let alert = UIAlertController(
            title: "ì¹´ë©”ë¼ ê¶Œí•œ í•„ìš”",
            message: "ì±…ì„ ì´¬ì˜í•˜ë ¤ë©´ ì„¤ì •ì—ì„œ ì¹´ë©”ë¼ ì ‘ê·¼ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.",
            preferredStyle: .alert
        )

        let settingsAction = UIAlertAction(title: "ì„¤ì •ìœ¼ë¡œ ì´ë™", style: .default) { _ in
            if let url = URL(string: UIApplication.openSettingsURLString) {
                UIApplication.shared.open(url, options: [:], completionHandler: nil)
            }
        }

        let cancelAction = UIAlertAction(title: "ì·¨ì†Œ", style: .cancel)

        alert.addAction(settingsAction)
        alert.addAction(cancelAction)

        DispatchQueue.main.async {
            self.present(alert, animated: true)
        }
    }

    private func bindUI() {
        captureButton.rx.tap
            .bind { [weak self] in
                self?.capturePhoto()
            }
            .disposed(by: disposeBag)
    }

    private func setupCamera() {
        DispatchQueue.global(qos: .userInitiated).async { [weak self] in
            guard let self else {
                return
            }
            guard let captureDevice = AVCaptureDevice.default(for: .video) else {
                print("ðŸš¨ ì¹´ë©”ë¼ ìž¥ì¹˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return
            }
            do {
                let input = try AVCaptureDeviceInput(device: captureDevice)
                captureSession.beginConfiguration()
                if captureSession.canAddInput(input) {
                    captureSession.addInput(input)
                }
                if captureSession.canAddOutput(captureOutput) {
                    captureSession.addOutput(captureOutput)
                }
                captureSession.commitConfiguration()

                DispatchQueue.main.async {
                    self.previewLayer = AVCaptureVideoPreviewLayer(session: self.captureSession)
                    self.previewLayer?.videoGravity = .resizeAspectFill
                    self.previewLayer?.frame = self.cameraView.bounds
                    self.cameraView.layer.insertSublayer(self.previewLayer!, at: 0)

                    // ë°±ê·¸ë¼ìš´ë“œì—ì„œ AVCaptureSession ì‹œìž‘
                    DispatchQueue.global(qos: .userInitiated).async {
                        if !self.captureSession.isRunning {
                            self.captureSession.startRunning()
                        }
                    }
                }
            } catch {
                print("ðŸš¨ ì¹´ë©”ë¼ ì´ˆê¸°í™” ì‹¤íŒ¨: \(error.localizedDescription)")
            }
        }
    }

    private func showCaptureFailurePopup() {
        let alert = UIAlertController(
            title: "ì´¬ì˜ ì‹¤íŒ¨",
            message: "ì´ë¯¸ì§€ë¥¼ ìº¡ì²˜í•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.",
            preferredStyle: .alert
        )

        let retryAction = UIAlertAction(title: "ìž¬ì‹œë„", style: .default) { _ in
            self.capturePhoto()
        }

        let cancelAction = UIAlertAction(title: "ì·¨ì†Œ", style: .cancel)

        alert.addAction(retryAction)
        alert.addAction(cancelAction)

        DispatchQueue.main.async {
            self.present(alert, animated: true)
        }
    }

    private func detectBookElements(in image: UIImage) {
        // CoreML ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•œì§€ í™•ì¸
        if let mlModel = MyObjectDetector5_1().model as? MLModel,
           mlModel.modelDescription.isUpdatable {
            print("âœ… ì´ ëª¨ë¸ì€ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
        } else {
            print("âš ï¸ ì´ ëª¨ë¸ì€ ì—…ë°ì´íŠ¸ê°€ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.")
        }

        // CoreML ëª¨ë¸ ë¡œë“œ
        guard let model = try? VNCoreMLModel(for: MyObjectDetector5_1().model) else {
            print("âš ï¸ CoreML ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: ëª¨ë¸ì´ ì—…ë°ì´íŠ¸ ê°€ëŠ¥í•œì§€ í™•ì¸ í•„ìš”")
            return
        }

        let request = VNCoreMLRequest(model: model) { request, error in
            if let error {
                print("âš ï¸ Vision ìš”ì²­ ì‹¤íŒ¨: \(error.localizedDescription)")
                return
            }

            guard let results = request.results as? [VNRecognizedObjectObservation] else {
                print("âš ï¸ Vision ê²°ê³¼ ì—†ìŒ")
                return
            }
            // TODO: RxSwiftë¡œ ë¹„ë™ê¸° ìž‘ì—… ì²˜ë¦¬ ë¡œì§ ë³€ê²½í•˜ê¸°

            var extractedTexts: [String] = []
            let dispatchGroup = DispatchGroup()

            for observation in results
                where observation.labels.first?.identifier == "titles-or-authors" {
                dispatchGroup.enter()
                self.performOCR(on: image) { recognizedText in
                    if !recognizedText.isEmpty {
                        extractedTexts.append(recognizedText)
                    }
                    dispatchGroup.leave()
                }
            }

            dispatchGroup.notify(queue: .main) {
                if let bestTitle = extractedTexts.first {
                    print("âœ… ìµœì¢… OCR ê²°ê³¼: \(bestTitle)")
                    self.ocrTextHandler?(bestTitle)
                } else {
                    print("âš ï¸ OCR ê²°ê³¼ ì—†ìŒ")
                }
            }
        }

        request.usesCPUOnly = true
        request.preferBackgroundProcessing = true

        do {
            let handler = VNImageRequestHandler(cgImage: image.cgImage!, options: [:])
            try handler.perform([request])
        } catch let error as NSError {
            print("âš ï¸ Vision Request Error: \(error.localizedDescription)")
        }
    }

    private func performOCR(on image: UIImage, completion: @escaping (String) -> Void) {
        guard let cgImage = image.cgImage else {
            print("âš ï¸ ì´ë¯¸ì§€ ë³€í™˜ ì‹¤íŒ¨")
            completion("")
            return
        }

        let request = VNRecognizeTextRequest { request, error in
            if let error {
                print("âš ï¸ OCR ì˜¤ë¥˜ ë°œìƒ: \(error.localizedDescription)")
                completion("")
                return
            }

            guard let observations = request.results as? [VNRecognizedTextObservation] else {
                print("âš ï¸ OCR ê²°ê³¼ ì—†ìŒ")
                completion("")
                return
            }

            let recognizedText = observations.compactMap { $0.topCandidates(1).first?.string }
                .joined(separator: " ")
            print("âœ… OCR ê²°ê³¼: \(recognizedText)")
            completion(recognizedText)
        }

        request.recognitionLevel = .accurate

        let requestHandler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        do {
            try requestHandler.perform([request])
        } catch {
            print("âš ï¸ OCR ìš”ì²­ ì‹¤íŒ¨: \(error.localizedDescription)")
            completion("")
        }
    }
}

// MARK: - UIImage Extension (ë¦¬ì‚¬ì´ì¦ˆ ê¸°ëŠ¥ ì¶”ê°€)

extension UIImage {
    func resized(toWidth width: CGFloat) -> UIImage? {
        let scaleFactor = width / size.width
        let canvasSize = CGSize(width: width, height: size.height * scaleFactor)

        UIGraphicsBeginImageContextWithOptions(canvasSize, false, UIScreen.main.scale)
        defer { UIGraphicsEndImageContext() }

        draw(in: CGRect(origin: .zero, size: canvasSize))
        return UIGraphicsGetImageFromCurrentImageContext()
    }
}
